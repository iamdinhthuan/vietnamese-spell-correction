# BARTpho Vietnamese Spelling Correction

Sá»­a lá»—i chÃ­nh táº£ tiáº¿ng Viá»‡t sá»­ dá»¥ng mÃ´ hÃ¬nh BARTpho fine-tuned trÃªn large-scale OCR correction dataset.

## ğŸŒŸ TÃ­nh nÄƒng

- âœ… **Large-scale training**: Há»— trá»£ streaming dataset vá»›i hÃ ng triá»‡u dÃ²ng mÃ  khÃ´ng cáº§n load toÃ n bá»™ vÃ o RAM
- âœ… **Resume training**: Tá»± Ä‘á»™ng tiáº¿p tá»¥c training tá»« checkpoint náº¿u bá»‹ giÃ¡n Ä‘oáº¡n
- âœ… **GPU optimization**: Tá»‘i Æ°u cho training vÃ  inference trÃªn GPU
- âœ… **Gradio UI**: Giao diá»‡n web thÃ¢n thiá»‡n Ä‘á»ƒ test model
- âœ… **Batch inference**: Há»— trá»£ xá»­ lÃ½ nhiá»u vÄƒn báº£n cÃ¹ng lÃºc

## ğŸ“‹ YÃªu cáº§u

```bash
pip install transformers datasets torch gradio
```

**PhiÃªn báº£n khuyáº¿n nghá»‹:**
- Python >= 3.8
- PyTorch >= 2.0
- transformers >= 4.30.0
- datasets >= 2.0.0

## ğŸ“Š Dataset

Dataset yÃªu cáº§u Ä‘á»‹nh dáº¡ng CSV vá»›i 2 cá»™t:
- `input_text`: VÄƒn báº£n cÃ³ lá»—i chÃ­nh táº£
- `target_text`: VÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c sá»­a lá»—i

**VÃ­ dá»¥:**
```csv
input_text,target_text
"Chay Ã¬ ná»™p phat nguá»™i.","Cháº¡y Ã¬ ná»‘p pháº¡t nguá»™i."
"ÄÃ¢y iÃ  cac phÆ°ong tiá»‡n vi pháº¡m","ÄÃ¢y lÃ  cÃ¡c phÆ°Æ¡ng tiá»‡n vi pháº¡m"
```

### Chuáº©n bá»‹ dataset

1. **Äáº·t file dataset gá»‘c** vá»›i tÃªn `ocr_correction_dataset.csv`

2. **Chia dataset thÃ nh train/val:**
```bash
python split_train_val.py
```

Script nÃ y sáº½:
- Shuffle toÃ n bá»™ dataset vá»›i seed cá»‘ Ä‘á»‹nh
- TÃ¡ch 200,000 dÃ²ng cho validation set
- Pháº§n cÃ²n láº¡i cho training set
- Output: `train.csv` vÃ  `val.csv`

## ğŸš€ Training

### Training tá»« Ä‘áº§u

```bash
python train_bartpho_streaming.py
```

### Resume training tá»« checkpoint

Náº¿u training bá»‹ giÃ¡n Ä‘oáº¡n, chá»‰ cáº§n cháº¡y láº¡i lá»‡nh trÃªn:
```bash
python train_bartpho_streaming.py
```

Script tá»± Ä‘á»™ng:
- TÃ¬m checkpoint má»›i nháº¥t trong `./bartpho_vsc/`
- Resume training tá»« checkpoint Ä‘Ã³
- Tiáº¿p tá»¥c vá»›i learning rate vÃ  optimizer state Ä‘Ã£ lÆ°u

**Output:**
- Checkpoints: `./bartpho_vsc/checkpoint-*/`
- Final model: `./bartpho_vsc_model/`

### Hyperparameters

CÃ³ thá»ƒ chá»‰nh sá»­a trong file `train_bartpho_streaming.py`:

```python
MAX_INPUT_LENGTH = 256        # Äá»™ dÃ i tá»‘i Ä‘a input
MAX_TARGET_LENGTH = 256       # Äá»™ dÃ i tá»‘i Ä‘a output
NUM_EPOCHS = 5                # Sá»‘ epoch
TRAIN_BATCH_SIZE = 64         # Batch size cho training
EVAL_BATCH_SIZE = 64          # Batch size cho evaluation
LEARNING_RATE = 5e-5          # Learning rate
WARMUP_STEPS = 10000          # Warmup steps
SAVE_STEPS = 10000            # LÆ°u checkpoint má»—i N steps
EVAL_STEPS = 10000            # Eval má»—i N steps
SHUFFLE_BUFFER = 50000        # Buffer size cho shuffle
NUM_WORKERS = 8               # Sá»‘ workers cho DataLoader
```

## ğŸ¯ Inference

### 1. Command-line inference

```bash
python inference_bartpho.py
```

**Sá»­ dá»¥ng trong code:**
```python
from inference_bartpho import correct_spelling, correct_batch

# Sá»­a 1 cÃ¢u
text = "Chay Ã¬ ná»™p phat nguá»™i"
corrected = correct_spelling(text, num_beams=5)
print(corrected)  # "Cháº¡y Ã¬ ná»‘p pháº¡t nguá»™i"

# Sá»­a nhiá»u cÃ¢u
texts = ["cÃ¢u 1 cÃ³ lá»—i", "cÃ¢u 2 cÃ³ lá»—i", "cÃ¢u 3 cÃ³ lá»—i"]
results = correct_batch(texts, batch_size=8, num_beams=5)
```

### 2. Gradio Web UI

```bash
python gradio_inference.py
```

Má»Ÿ trÃ¬nh duyá»‡t táº¡i: **http://localhost:7860**

**TÃ­nh nÄƒng UI:**
- Input/output textbox
- Äiá»u chá»‰nh `num_beams` (1-10) báº±ng slider
- 5 vÃ­ dá»¥ máº«u Ä‘á»ƒ test nhanh
- Tá»± Ä‘á»™ng load checkpoint má»›i nháº¥t tá»« `/home/thuan/data/bartpho_vsc/`

## ğŸ“‚ Cáº¥u trÃºc thÆ° má»¥c

```
.
â”œâ”€â”€ train_bartpho_streaming.py    # Script training chÃ­nh
â”œâ”€â”€ inference_bartpho.py           # CLI inference
â”œâ”€â”€ gradio_inference.py            # Gradio web UI
â”œâ”€â”€ split_train_val.py             # Chia dataset
â”œâ”€â”€ ocr_correction_dataset.csv     # Dataset gá»‘c (khÃ´ng commit)
â”œâ”€â”€ train.csv                      # Training set (khÃ´ng commit)
â”œâ”€â”€ val.csv                        # Validation set (khÃ´ng commit)
â”œâ”€â”€ bartpho_vsc/                   # Checkpoints (khÃ´ng commit)
â”‚   â”œâ”€â”€ checkpoint-10000/
â”‚   â”œâ”€â”€ checkpoint-20000/
â”‚   â””â”€â”€ ...
â””â”€â”€ bartpho_vsc_model/             # Final model (khÃ´ng commit)
    â”œâ”€â”€ config.json
    â”œâ”€â”€ pytorch_model.bin
    â””â”€â”€ tokenizer files
```

## ğŸ¨ VÃ­ dá»¥

```python
# Example 1: Sá»­a lá»—i dáº¥u thanh vÃ  dáº¥u phá»¥
input  = "Chay Ã¬ ná»™p phat nguá»™i."
output = "Cháº¡y Ã¬ ná»‘p pháº¡t nguá»™i."

# Example 2: Sá»­a lá»—i chá»¯ cÃ¡i
input  = "ÄÃ¢y iÃ  cac phÆ°ong tiá»‡n vi pháº¡m Ä‘Æ°á»£c camera ghi hÃ¬nh."
output = "ÄÃ¢y lÃ  cÃ¡c phÆ°Æ¡ng tiá»‡n vi pháº¡m Ä‘Æ°á»£c camera ghi hÃ¬nh."

# Example 3: Sá»­a lá»—i tá»«
input  = "Phá»• biáº¿n nhat iÃ  ioi Ä‘á»— khÃ´ng Ä‘Ãºng nÆ¡i quy dá»‹nh."
output = "Phá»• biáº¿n nháº¥t lÃ  lá»—i Ä‘á»— khÃ´ng Ä‘Ãºng nÆ¡i quy Ä‘á»‹nh."

# Example 4: Sá»­a lá»—i phá»©c táº¡p
input  = "TÃ´i Ä‘ang há»c tap tiÃ©ng viÃ©t Æ¡ trÆ°Ã²ng Ä‘ai hoc."
output = "TÃ´i Ä‘ang há»c táº­p tiáº¿ng viá»‡t á»Ÿ trÆ°á»ng Ä‘áº¡i há»c."
```

## âš™ï¸ Cáº¥u hÃ¬nh cho mÃ´i trÆ°á»ng khÃ¡c

### Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n checkpoint

**Trong `gradio_inference.py`:**
```python
CHECKPOINT_DIR = "/home/thuan/data/bartpho_vsc/"  # Äá»•i Ä‘Æ°á»ng dáº«n nÃ y
```

**Trong `inference_bartpho.py`:**
```python
MODEL_DIR = "./bartpho_vsc_model"  # Äá»•i Ä‘Æ°á»ng dáº«n nÃ y
```

### Training trÃªn nhiá»u GPU

Sá»­ dá»¥ng `torchrun` hoáº·c `accelerate`:

```bash
# Vá»›i torchrun (PyTorch distributed)
torchrun --nproc_per_node=2 train_bartpho_streaming.py

# Vá»›i accelerate
accelerate config  # Cáº¥u hÃ¬nh 1 láº§n
accelerate launch train_bartpho_streaming.py
```

## ğŸ“ˆ Monitoring Training

Trong quÃ¡ trÃ¬nh training, log sáº½ hiá»ƒn thá»‹:
- Loss value má»—i `LOGGING_STEPS` (máº·c Ä‘á»‹nh: 2000 steps)
- Evaluation metrics má»—i `EVAL_STEPS` (máº·c Ä‘á»‹nh: 10000 steps)
- Checkpoint Ä‘Æ°á»£c lÆ°u má»—i `SAVE_STEPS` (máº·c Ä‘á»‹nh: 10000 steps)

**VÃ­ dá»¥ log:**
```
{'loss': 0.5234, 'learning_rate': 4.5e-05, 'epoch': 0.5, 'step': 10000}
{'eval_loss': 0.4123, 'eval_runtime': 245.3, 'eval_samples_per_second': 815.2}
```

## ğŸ› Troubleshooting

### 1. Out of Memory (OOM)

Giáº£m batch size trong `train_bartpho_streaming.py`:
```python
TRAIN_BATCH_SIZE = 32  # Giáº£m tá»« 64
EVAL_BATCH_SIZE = 32
```

### 2. Training cháº­m

TÄƒng sá»‘ workers vÃ  prefetch:
```python
NUM_WORKERS = 16               # TÄƒng workers
PREFETCH_FACTOR = 8            # TÄƒng prefetch
TOKENIZATION_BATCH_SIZE = 2000 # TÄƒng batch size tokenization
```

### 3. Checkpoint khÃ´ng load

Kiá»ƒm tra Ä‘Æ°á»ng dáº«n:
```bash
ls -la ./bartpho_vsc/checkpoint-*
```

### 4. Gradio khÃ´ng káº¿t ná»‘i

Thá»­ port khÃ¡c:
```python
demo.launch(server_port=7861)  # Äá»•i port
```

## ğŸ“ Model Information

- **Base model**: [`vinai/bartpho-syllable-base`](https://huggingface.co/vinai/bartpho-syllable-base)
- **Task**: Seq2Seq text correction
- **Architecture**: BART (Bidirectional and Auto-Regressive Transformers)
- **Tokenization**: Syllable-level tokenization cho tiáº¿ng Viá»‡t
- **Parameters**: ~135M parameters

## ğŸ“„ License

MIT License

## ğŸ™ Acknowledgments

- [VinAI Research](https://github.com/VinAIResearch) - BARTpho model
- [Hugging Face](https://huggingface.co/) - Transformers library

## ğŸ“§ Contact

Náº¿u cÃ³ váº¥n Ä‘á» hoáº·c cÃ¢u há»i, vui lÃ²ng táº¡o issue trÃªn GitHub.

---

**Happy spell checking! ğŸ‰**
